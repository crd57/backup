keras.layers.LSTM(
units, 输出维度 
activation='tanh',#激活函数tanh
recurrent_activation='hard_sigmoid', 为循环步施加的激活函数
use_bias=True, 布尔值，是否使用偏置项
kernel_initializer='glorot_uniform', 权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器
recurrent_initializer='orthogonal', 循环核的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。
bias_initializer='zeros', 权值初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。
unit_forget_bias=True, 
kernel_regularizer=None, 施加在输出上的正则项，为Regularizer对象
recurrent_regularizer=None, 施加在循环步上的正则项
bias_regularizer=None, 施加在偏置向量上的正则项
activity_regularizer=None, 施加在输出上的正则项
kernel_constraint=None, 施加在权重上的约束项
recurrent_constraint=None, 施加在循环核上的约束项
bias_constraint=None, 施加在偏置上的约束项
dropout=0.0, 0~1之间的浮点数，控制输入线性变换的神经元断开比例
recurrent_dropout=0.0, 0~1之间的浮点数，控制循环状态的线性变换的神经元断开比例
implementation=1, 0，1或2， 若为0，则RNN将以更少但是更大的矩阵乘法实现，因此在CPU上运行更快，但消耗更多的内存。如果设为1，则RNN将以更多但更小的矩阵乘法实现，因此在CPU上运行更慢，在GPU上运行更快，并且消耗更少的内存。如果设为2（仅LSTM和GRU可以设为2），则RNN将把输入门、遗忘门和输出门合并为单个矩阵，以获得更加在GPU上更加高效的实现。注意，RNN dropout必须在所有门上共享，并导致正则效果性能微弱降低。
return_sequences=False, 布尔值，默认False，控制返回类型。若为True则返回整个序列，否则仅返回输出序列的最后一个输出
return_state=False, 
go_backwards=False, 布尔值，默认为False，若为True，则逆向处理输入序列并返回逆序后的序列
stateful=False, 布尔值，默认为False，若为True，则一个batch中下标为i的样本的最终状态将会用作下一个batch同样下标的样本的初始状态。
unroll=False)布尔值，默认为False，若为True，则循环层将被展开，否则就使用符号化的循环。当使用TensorFlow为后端时，循环网络本来就是展开的，因此该层不做任何事情。层展开会占用更多的内存，但会加速RNN的运算。层展开只适用于短序列。